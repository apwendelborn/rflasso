\name{General Discussion}
\alias{General Discussion}

\title{General Discussion}
\details{
To download this package, first download the devtools library and load it. Next, run install_github("apwendelborn/rflasso").

The general purpose of this package is to provide a model which improves upon random forest. Currently, this package only supports regression (no classification). In a standard random forest, predictions are made by finding the terminal nodes associated with a new data point. The dependent variable for all of these terminal nodes is then averaged to make a prediction. rflasso performs a similar operation except instead of averaging the dependent variable and discarding all the information on the independent variables, a lasso model is fit to the pooled data and predictions are made from this linear model. In effect, applying lasso to the data in the terminal nodes is like fitting a kernel regression model where the kernel weights are determined by the random forest. Lasso is leveraged because it can handle singular matrices and provides variable selection to return a sparse set of coefficients. When fitting lasso models, this package utilizes glmnet's "weight" parameter and counting the number of times an observation is in the pooled terminal nodes to use as the weight.

See the documentation on the four functions associated with this package:

1. build.tree - This is used to build a random forest object which is compatible with the other functions.

2. out.of.bag.error - This calculates the out of bag RMSE error of the random forest generated by either build.tree or auto.optimize. The returned RMSE value is a good predictor of out of sample error because the predictions are only made using trees from the ensemble which do not include the current sample.

3. predict.lm.from.rf - This function is used to make predictions from the random forests generated by either build.tree or auto.optimize. The predictions.only parameter will allow the lasso linear model coefficients to be returned. This could potentially be useful for inference purposes (it answers: based on similar observations, how do changes in the independent variables effect the dependent variable?).

4. auto.optimize - This function automatically optimizes the random forest objects by pruning the trees using a penalty term ("signficance"):

[(MSE of Child 1)*(Number of samples in Child 1 - 1)+(MSE of Child 2)*(Number of samples in Child 2 - 1)]*(Significance) (> or <) [(MSE of Parent)*(Number of samples in Parent - 1)]

Once pruning is complete, out.of.bag.error is run. The function moves through multiple values of "significance" controlled by "guess" and "step.size" and compares the out of bag errors. Once a saddle/minimum point is identified, the function returns a pruned tree object. Please note multiple local minimums can be found.

out.of.bag.error and predict.lm.from.rf have two models to choose from: "average" and "lasso". It is important to run both models with the out.of.bag.error function to determine whether lasso actually improves the out of bag error. Lasso is not always the preferred model.

In all cases, lasso regularization is optimized by cross validation with 7 folds using the glmnet package.

Please note:

1. This package cannot handle missing values.

2. If you create too many trees or have too many samples per tree, R can run into memory problems and the tree object will not be created.
}

\author{
Alexander Wendelborn
<apwendelborn@gmail.com>
}

